{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n-Ps2nkVsBb"
      },
      "source": [
        "# **💁🏻🗨️💁🏻‍♂️대화 요약 Baseline code**\n",
        "> **Dialogue Summarization** 경진대회에 오신 여러분 환영합니다! 🎉    \n",
        "> 본 대회에서는 최소 2명에서 최대 7명이 등장하여 나누는 대화를 요약하는 BART 기반 모델의 baseline code를 제공합니다.     \n",
        "> 주어진 데이터를 활용하여 일상 대화에 대한 요약을 효과적으로 생성하는 모델을 만들어봅시다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNq_LylZa1ug"
      },
      "source": [
        "## ⚙️ 데이터 및 환경설정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjCiuI_V4glr"
      },
      "source": [
        "### 1) 필요한 라이브러리 설치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYqDF_-r2ToB"
      },
      "source": [
        "- 필요한 라이브러리를 설치한 후 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbZ7SU9P2TYN",
        "outputId": "f256a02b-f012-4063-cef3-1ebd2e286620",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pytorch_lightning'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-39fad13b5886>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrouge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRouge\u001b[0m \u001b[0;31m# 모델의 성능을 평가하기 위한 라이브러리입니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pytorch_lightning'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import yaml\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from rouge import Rouge # 모델의 성능을 평가하기 위한 라이브러리입니다.\n",
        "\n",
        "from torch.utils.data import Dataset , DataLoader\n",
        "from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "import wandb # 모델 학습 과정을 손쉽게 Tracking하고, 시각화할 수 있는 라이브러리입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Qq46k6_CNQn"
      },
      "source": [
        "### 2) Config file 만들기 (선택)\n",
        "- 모델 생성에 필요한 다양한 매개변수 정보를 저장할 수 있습니다.  \n",
        "  따라서, 코드 상에서 모델의 매개변수를 설정할 수도 있지만 독립적인 매개변수 정보 파일을 생성하여 관리할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZOE9TInCQHJ"
      },
      "outputs": [],
      "source": [
        "# config 설정에 tokenizer 모듈이 사용되므로 미리 tokenizer를 정의해줍니다.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vsACJI7CVb8"
      },
      "outputs": [],
      "source": [
        "config_data = {\n",
        "    \"general\": {\n",
        "        \"data_path\": \"../data/\", # 모델 생성에 필요한 데이터 경로를 사용자 환경에 맞게 지정합니다.\n",
        "        \"model_name\": \"digit82/kobart-summarization\", # 불러올 모델의 이름을 사용자 환경에 맞게 지정할 수 있습니다.\n",
        "        \"output_dir\": \"./\" # 모델의 최종 출력 값을 저장할 경로를 설정합니다.\n",
        "    },\n",
        "    \"tokenizer\": {\n",
        "        \"encoder_max_len\": 512,\n",
        "        \"decoder_max_len\": 100,\n",
        "        \"bos_token\": f\"{tokenizer.bos_token}\",\n",
        "        \"eos_token\": f\"{tokenizer.eos_token}\",\n",
        "        # 특정 단어들이 분해되어 tokenization이 수행되지 않도록 special_tokens을 지정해줍니다.\n",
        "        \"special_tokens\": ['#Person1#', '#Person2#', '#Person3#', '#PhoneNumber#', '#Address#', '#PassportNumber#']\n",
        "    },\n",
        "    \"training\": {\n",
        "        \"overwrite_output_dir\": True,\n",
        "        \"num_train_epochs\": 20,\n",
        "        \"learning_rate\": 1e-5,\n",
        "        \"per_device_train_batch_size\": 50,\n",
        "        \"per_device_eval_batch_size\": 32,\n",
        "        \"warmup_ratio\": 0.1,\n",
        "        \"weight_decay\": 0.01,\n",
        "        \"lr_scheduler_type\": 'cosine',\n",
        "        \"optim\": 'adamw_torch',\n",
        "        \"gradient_accumulation_steps\": 1,\n",
        "        \"evaluation_strategy\": 'epoch',\n",
        "        \"save_strategy\": 'epoch',\n",
        "        \"save_total_limit\": 5,\n",
        "        \"fp16\": True,\n",
        "        \"load_best_model_at_end\": True,\n",
        "        \"seed\": 42,\n",
        "        \"logging_dir\": \"./logs\",\n",
        "        \"logging_strategy\": \"epoch\",\n",
        "        \"predict_with_generate\": True,\n",
        "        \"generation_max_length\": 100,\n",
        "        \"do_train\": True,\n",
        "        \"do_eval\": True,\n",
        "        \"early_stopping_patience\": 3,\n",
        "        \"early_stopping_threshold\": 0.001,\n",
        "        \"report_to\": \"wandb\" # (선택) wandb를 사용할 때 설정합니다.\n",
        "    },\n",
        "    # (선택) wandb 홈페이지에 가입하여 얻은 정보를 기반으로 작성합니다.\n",
        "    \"wandb\": {\n",
        "        \"entity\": \"wandb_repo\",\n",
        "        \"project\": \"project_name\",\n",
        "        \"name\": \"run_name\"\n",
        "    },\n",
        "    \"inference\": {\n",
        "        \"ckt_path\": \"model ckt path\", # 사전 학습이 진행된 모델의 checkpoint를 저장할 경로를 설정합니다.\n",
        "        \"result_path\": \"./prediction/\",\n",
        "        \"no_repeat_ngram_size\": 2,\n",
        "        \"early_stopping\": True,\n",
        "        \"generate_max_length\": 100,\n",
        "        \"num_beams\": 4,\n",
        "        \"batch_size\" : 32,\n",
        "        # 정확한 모델 평가를 위해 제거할 불필요한 생성 토큰들을 정의합니다.\n",
        "        \"remove_tokens\": ['<usr>', f\"{tokenizer.bos_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cm7ob25lHBkR"
      },
      "source": [
        "- 참고✅    \n",
        ": wandb 라이브러리를 사용하기 위해선 entity, project, name를 지정해주어야 합니다. wandb 홈페이지에 가입한 후 얻은 정보를 입력하여 작동할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REJybO5UCabF"
      },
      "outputs": [],
      "source": [
        "# 모델의 구성 정보를 YAML 파일로 저장합니다.\n",
        "config_path = \"./config.yaml\"\n",
        "with open(config_path, \"w\") as file:\n",
        "    yaml.dump(config_data, file, allow_unicode=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObEASD6Wj6pl"
      },
      "source": [
        "### 3) Configuration 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUBm_6RqlYpV"
      },
      "outputs": [],
      "source": [
        "# 저장된 config 파일을 불러옵니다.\n",
        "config_path = \"./config.yaml\"\n",
        "\n",
        "with open(config_path, \"r\") as file:\n",
        "    loaded_config = yaml.safe_load(file)\n",
        "\n",
        "# 불러온 config 파일의 전체 내용을 확인합니다.\n",
        "pprint(loaded_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRSbKEVslhwO"
      },
      "outputs": [],
      "source": [
        "# 실험에 쓰일 데이터의 경로, 사용될 모델, 모델의 최종 출력 결과를 저장할 경로에 대해 확인합니다.\n",
        "loaded_config['general']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aiNuy4qhH03"
      },
      "outputs": [],
      "source": [
        "# 이곳에 사용자가 저장한 데이터 dir 설정하기\n",
        "# loaded_config['general']['data_path'] = \"data_path\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pvFmIOqljv1"
      },
      "outputs": [],
      "source": [
        "# 데이터 전처리를 하기 위해 tokenization 과정에서 필요한 정보들을 확인합니다.\n",
        "loaded_config['tokenizer']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEvwCIBVll-h"
      },
      "outputs": [],
      "source": [
        "# 모델이 훈련 시 적용될 매개변수를 확인합니다.\n",
        "loaded_config['training']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhqHf1njlnyg"
      },
      "outputs": [],
      "source": [
        "# 모델 학습 과정에 대한 정보를 제공해주는 wandb 설정 내용을 확인합니다.\n",
        "loaded_config['wandb']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aVN0kwlWhH09"
      },
      "outputs": [],
      "source": [
        "# (선택) 이곳에 사용자가 사용할 wandb config 설정\n",
        "loaded_config['wandb']['entity'] = \"사용할 wandb repo name\"\n",
        "loaded_config['wandb']['name'] = \"사용할 wandb run의 name\"\n",
        "loaded_config['wandb']['project'] = \"사용할 wandb project name\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm4gxPRVlppj"
      },
      "outputs": [],
      "source": [
        "# 모델이 최종 결과를 출력하기 위한 매개변수 정보를 확인합니다.\n",
        "loaded_config['inference']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2zt0b-8ogCL"
      },
      "source": [
        "### 4) 데이터 불러와서 확인해보기\n",
        "- 실험에서 쓰일 데이터를 load하여 데이터의 구조와 내용을 살펴보겠습니다.\n",
        "- Train, dev, test 순서대로 12457, 499, 250개 씩 데이터가 구성되어 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFHIE2G04y-K"
      },
      "outputs": [],
      "source": [
        "# config에 저장된 데이터 경로를 통해 train과 validation data를 불러옵니다.\n",
        "data_path = loaded_config['general']['data_path']\n",
        "\n",
        "# train data의 구조와 내용을 확인합니다.\n",
        "train_df = pd.read_csv(os.path.join(data_path,'train.csv'))\n",
        "train_df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAGaYvNZ09Sq"
      },
      "outputs": [],
      "source": [
        "# validation data의 구조와 내용을 확인합니다.\n",
        "val_df = pd.read_csv(os.path.join(data_path,'dev.csv'))\n",
        "val_df.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IIaIrpH4kWo"
      },
      "source": [
        "## 1. 데이터 가공 및 데이터셋 클래스 구축\n",
        "- csv file 을 불러와서 encoder 와 decoder의 입력형태로 가공해줍니다.\n",
        "- 가공된 데이터를 torch dataset class 로 구축하여 모델에 입력가능한 형태로 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWPawUUflwHa"
      },
      "outputs": [],
      "source": [
        "# 데이터 전처리를 위한 클래스로, 데이터셋을 데이터프레임으로 변환하고 인코더와 디코더의 입력을 생성합니다.\n",
        "class Preprocess:\n",
        "    def __init__(self,\n",
        "            bos_token: str,\n",
        "            eos_token: str,\n",
        "        ) -> None:\n",
        "\n",
        "        self.bos_token = bos_token\n",
        "        self.eos_token = eos_token\n",
        "\n",
        "    @staticmethod\n",
        "    # 실험에 필요한 컬럼을 가져옵니다.\n",
        "    def make_set_as_df(file_path, is_train = True):\n",
        "        if is_train:\n",
        "            df = pd.read_csv(file_path)\n",
        "            train_df = df[['fname','dialogue','summary']]\n",
        "            return train_df\n",
        "        else:\n",
        "            df = pd.read_csv(file_path)\n",
        "            test_df = df[['fname','dialogue']]\n",
        "            return test_df\n",
        "\n",
        "    # BART 모델의 입력, 출력 형태를 맞추기 위해 전처리를 진행합니다.\n",
        "    def make_input(self, dataset,is_test = False):\n",
        "        if is_test:\n",
        "            encoder_input = dataset['dialogue']\n",
        "            decoder_input = [self.bos_token] * len(dataset['dialogue'])\n",
        "            return encoder_input.tolist(), list(decoder_input)\n",
        "        else:\n",
        "            encoder_input = dataset['dialogue']\n",
        "            decoder_input = dataset['summary'].apply(lambda x : self.bos_token + str(x)) # Ground truth를 디코더의 input으로 사용하여 학습합니다.\n",
        "            decoder_output = dataset['summary'].apply(lambda x : str(x) + self.eos_token)\n",
        "            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GDvodoF8sED"
      },
      "outputs": [],
      "source": [
        "# Train에 사용되는 Dataset 클래스를 정의합니다.\n",
        "class DatasetForTrain(Dataset):\n",
        "    def __init__(self, encoder_input, decoder_input, labels, len):\n",
        "        self.encoder_input = encoder_input\n",
        "        self.decoder_input = decoder_input\n",
        "        self.labels = labels\n",
        "        self.len = len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()} # item[input_ids], item[attention_mask]\n",
        "        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()} # item2[input_ids], item2[attention_mask]\n",
        "        item2['decoder_input_ids'] = item2['input_ids']\n",
        "        item2['decoder_attention_mask'] = item2['attention_mask']\n",
        "        item2.pop('input_ids')\n",
        "        item2.pop('attention_mask')\n",
        "        item.update(item2) #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask]\n",
        "        item['labels'] = self.labels['input_ids'][idx] #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "# Validation에 사용되는 Dataset 클래스를 정의합니다.\n",
        "class DatasetForVal(Dataset):\n",
        "    def __init__(self, encoder_input, decoder_input, labels, len):\n",
        "        self.encoder_input = encoder_input\n",
        "        self.decoder_input = decoder_input\n",
        "        self.labels = labels\n",
        "        self.len = len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()} # item[input_ids], item[attention_mask]\n",
        "        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()} # item2[input_ids], item2[attention_mask]\n",
        "        item2['decoder_input_ids'] = item2['input_ids']\n",
        "        item2['decoder_attention_mask'] = item2['attention_mask']\n",
        "        item2.pop('input_ids')\n",
        "        item2.pop('attention_mask')\n",
        "        item.update(item2) #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask]\n",
        "        item['labels'] = self.labels['input_ids'][idx] #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "# Test에 사용되는 Dataset 클래스를 정의합니다.\n",
        "class DatasetForInference(Dataset):\n",
        "    def __init__(self, encoder_input, test_id, len):\n",
        "        self.encoder_input = encoder_input\n",
        "        self.test_id = test_id\n",
        "        self.len = len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
        "        item['ID'] = self.test_id[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hT9z4vvS2CCb"
      },
      "outputs": [],
      "source": [
        "# tokenization 과정까지 진행된 최종적으로 모델에 입력될 데이터를 출력합니다.\n",
        "def prepare_train_dataset(config, preprocessor, data_path, tokenizer):\n",
        "    train_file_path = os.path.join(data_path,'train.csv')\n",
        "    val_file_path = os.path.join(data_path,'dev.csv')\n",
        "\n",
        "    # train, validation에 대해 각각 데이터프레임을 구축합니다.\n",
        "    train_data = preprocessor.make_set_as_df(train_file_path)\n",
        "    val_data = preprocessor.make_set_as_df(val_file_path)\n",
        "\n",
        "    print('-'*150)\n",
        "    print(f'train_data:\\n {train_data[\"dialogue\"][0]}')\n",
        "    print(f'train_label:\\n {train_data[\"summary\"][0]}')\n",
        "\n",
        "    print('-'*150)\n",
        "    print(f'val_data:\\n {val_data[\"dialogue\"][0]}')\n",
        "    print(f'val_label:\\n {val_data[\"summary\"][0]}')\n",
        "\n",
        "    encoder_input_train , decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)\n",
        "    encoder_input_val , decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)\n",
        "    print('-'*10, 'Load data complete', '-'*10,)\n",
        "\n",
        "    tokenized_encoder_inputs = tokenizer(encoder_input_train, return_tensors=\"pt\", padding=True,\n",
        "                            add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
        "    tokenized_decoder_inputs = tokenizer(decoder_input_train, return_tensors=\"pt\", padding=True,\n",
        "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
        "    tokenized_decoder_ouputs = tokenizer(decoder_output_train, return_tensors=\"pt\", padding=True,\n",
        "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
        "\n",
        "    train_inputs_dataset = DatasetForTrain(tokenized_encoder_inputs, tokenized_decoder_inputs, tokenized_decoder_ouputs,len(encoder_input_train))\n",
        "\n",
        "    val_tokenized_encoder_inputs = tokenizer(encoder_input_val, return_tensors=\"pt\", padding=True,\n",
        "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False)\n",
        "    val_tokenized_decoder_inputs = tokenizer(decoder_input_val, return_tensors=\"pt\", padding=True,\n",
        "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
        "    val_tokenized_decoder_ouputs = tokenizer(decoder_output_val, return_tensors=\"pt\", padding=True,\n",
        "                        add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False)\n",
        "\n",
        "    val_inputs_dataset = DatasetForVal(val_tokenized_encoder_inputs, val_tokenized_decoder_inputs, val_tokenized_decoder_ouputs,len(encoder_input_val))\n",
        "\n",
        "    print('-'*10, 'Make dataset complete', '-'*10,)\n",
        "    return train_inputs_dataset, val_inputs_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5sKIJ5K5Pz1"
      },
      "source": [
        "## 2. Trainer 및 Trainingargs 구축하기\n",
        "- Huggingface 의 Trainer 와 Training arguments를 활용하여 모델 학습을 일괄적으로 처리해주는 클래스를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQk8ILcEeGNz"
      },
      "outputs": [],
      "source": [
        "# 모델 성능에 대한 평가 지표를 정의합니다. 본 대회에서는 ROUGE 점수를 통해 모델의 성능을 평가합니다.\n",
        "def compute_metrics(config,tokenizer,pred):\n",
        "    rouge = Rouge()\n",
        "    predictions = pred.predictions\n",
        "    labels = pred.label_ids\n",
        "\n",
        "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
        "    labels[labels == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n",
        "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # 정확한 평가를 위해 미리 정의된 불필요한 생성토큰들을 제거합니다.\n",
        "    replaced_predictions = decoded_preds.copy()\n",
        "    replaced_labels = labels.copy()\n",
        "    remove_tokens = config['inference']['remove_tokens']\n",
        "    for token in remove_tokens:\n",
        "        replaced_predictions = [sentence.replace(token,\" \") for sentence in replaced_predictions]\n",
        "        replaced_labels = [sentence.replace(token,\" \") for sentence in replaced_labels]\n",
        "\n",
        "    print('-'*150)\n",
        "    print(f\"PRED: {replaced_predictions[0]}\")\n",
        "    print(f\"GOLD: {replaced_labels[0]}\")\n",
        "    print('-'*150)\n",
        "    print(f\"PRED: {replaced_predictions[1]}\")\n",
        "    print(f\"GOLD: {replaced_labels[1]}\")\n",
        "    print('-'*150)\n",
        "    print(f\"PRED: {replaced_predictions[2]}\")\n",
        "    print(f\"GOLD: {replaced_labels[2]}\")\n",
        "\n",
        "    # 최종적인 ROUGE 점수를 계산합니다.\n",
        "    results = rouge.get_scores(replaced_predictions, replaced_labels,avg=True)\n",
        "\n",
        "    # ROUGE 점수 중 F-1 score를 통해 평가합니다.\n",
        "    result = {key: value[\"f\"] for key, value in results.items()}\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RInkG8g-HjBi"
      },
      "outputs": [],
      "source": [
        "# 학습을 위한 trainer 클래스와 매개변수를 정의합니다.\n",
        "def load_trainer_for_train(config,generate_model,tokenizer,train_inputs_dataset,val_inputs_dataset):\n",
        "    print('-'*10, 'Make training arguments', '-'*10,)\n",
        "    # set training args\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "                output_dir=config['general']['output_dir'], # model output directory\n",
        "                overwrite_output_dir=config['training']['overwrite_output_dir'],\n",
        "                num_train_epochs=config['training']['num_train_epochs'],  # total number of training epochs\n",
        "                learning_rate=config['training']['learning_rate'], # learning_rate\n",
        "                per_device_train_batch_size=config['training']['per_device_train_batch_size'], # batch size per device during training\n",
        "                per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],# batch size for evaluation\n",
        "                warmup_ratio=config['training']['warmup_ratio'],  # number of warmup steps for learning rate scheduler\n",
        "                weight_decay=config['training']['weight_decay'],  # strength of weight decay\n",
        "                lr_scheduler_type=config['training']['lr_scheduler_type'],\n",
        "                optim =config['training']['optim'],\n",
        "                gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],\n",
        "                evaluation_strategy=config['training']['evaluation_strategy'], # evaluation strategy to adopt during training\n",
        "                save_strategy =config['training']['save_strategy'],\n",
        "                save_total_limit=config['training']['save_total_limit'], # number of total save model.\n",
        "                fp16=config['training']['fp16'],\n",
        "                load_best_model_at_end=config['training']['load_best_model_at_end'], # 최종적으로 가장 높은 점수 저장\n",
        "                seed=config['training']['seed'],\n",
        "                logging_dir=config['training']['logging_dir'], # directory for storing logs\n",
        "                logging_strategy=config['training']['logging_strategy'],\n",
        "                predict_with_generate=config['training']['predict_with_generate'], #To use BLEU or ROUGE score\n",
        "                generation_max_length=config['training']['generation_max_length'],\n",
        "                do_train=config['training']['do_train'],\n",
        "                do_eval=config['training']['do_eval'],\n",
        "                report_to=config['training']['report_to'] # (선택) wandb를 사용할 때 설정합니다.\n",
        "            )\n",
        "\n",
        "    # (선택) 모델의 학습 과정을 추적하는 wandb를 사용하기 위해 초기화 해줍니다.\n",
        "    wandb.init(\n",
        "        entity=config['wandb']['entity'],\n",
        "        project=config['wandb']['project'],\n",
        "        name=config['wandb']['name'],\n",
        "    )\n",
        "\n",
        "    # (선택) 모델 checkpoint를 wandb에 저장하도록 환경 변수를 설정합니다.\n",
        "    os.environ[\"WANDB_LOG_MODEL\"]=\"true\"\n",
        "    os.environ[\"WANDB_WATCH\"]=\"false\"\n",
        "\n",
        "    # Validation loss가 더 이상 개선되지 않을 때 학습을 중단시키는 EarlyStopping 기능을 사용합니다.\n",
        "    MyCallback = EarlyStoppingCallback(\n",
        "        early_stopping_patience=config['training']['early_stopping_patience'],\n",
        "        early_stopping_threshold=config['training']['early_stopping_threshold']\n",
        "    )\n",
        "    print('-'*10, 'Make training arguments complete', '-'*10,)\n",
        "    print('-'*10, 'Make trainer', '-'*10,)\n",
        "\n",
        "    # Trainer 클래스를 정의합니다.\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=generate_model, # 사용자가 사전 학습하기 위해 사용할 모델을 입력합니다.\n",
        "        args=training_args,\n",
        "        train_dataset=train_inputs_dataset,\n",
        "        eval_dataset=val_inputs_dataset,\n",
        "        compute_metrics = lambda pred: compute_metrics(config,tokenizer, pred),\n",
        "        callbacks = [MyCallback]\n",
        "    )\n",
        "    print('-'*10, 'Make trainer complete', '-'*10,)\n",
        "\n",
        "    return trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKWHe8dE5fSx"
      },
      "outputs": [],
      "source": [
        "# 학습을 위한 tokenizer와 사전 학습된 모델을 불러옵니다.\n",
        "def load_tokenizer_and_model_for_train(config,device):\n",
        "    print('-'*10, 'Load tokenizer & model', '-'*10,)\n",
        "    print('-'*10, f'Model Name : {config[\"general\"][\"model_name\"]}', '-'*10,)\n",
        "    model_name = config['general']['model_name']\n",
        "    bart_config = BartConfig().from_pretrained(model_name)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    generate_model = BartForConditionalGeneration.from_pretrained(config['general']['model_name'],config=bart_config)\n",
        "\n",
        "    special_tokens_dict={'additional_special_tokens':config['tokenizer']['special_tokens']}\n",
        "    tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "    generate_model.resize_token_embeddings(len(tokenizer)) # 사전에 special token을 추가했으므로 재구성 해줍니다.\n",
        "    generate_model.to(device)\n",
        "    print(generate_model.config)\n",
        "\n",
        "    print('-'*10, 'Load tokenizer & model complete', '-'*10,)\n",
        "    return generate_model , tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvutzKQYvQgl"
      },
      "source": [
        "## 3. 모델 학습하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImZUb-BC42J-"
      },
      "source": [
        "- 앞에서 구축한 클래스 및 함수를 활용하여 학습 진행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnA96wmR44is"
      },
      "outputs": [],
      "source": [
        "def main(config):\n",
        "    # 사용할 device를 정의합니다.\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available()  else 'cpu')\n",
        "    print('-'*10, f'device : {device}', '-'*10,)\n",
        "    print(torch.__version__)\n",
        "\n",
        "    # 사용할 모델과 tokenizer를 불러옵니다.\n",
        "    generate_model , tokenizer = load_tokenizer_and_model_for_train(config,device)\n",
        "    print('-'*10,\"tokenizer special tokens : \",tokenizer.special_tokens_map,'-'*10)\n",
        "\n",
        "    # 학습에 사용할 데이터셋을 불러옵니다.\n",
        "    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token']) # decoder_start_token: str, eos_token: str\n",
        "    data_path = config['general']['data_path']\n",
        "    train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config,preprocessor, data_path, tokenizer)\n",
        "\n",
        "    # Trainer 클래스를 불러옵니다.\n",
        "    trainer = load_trainer_for_train(config, generate_model,tokenizer,train_inputs_dataset,val_inputs_dataset)\n",
        "    trainer.train()   # 모델 학습을 시작합니다.\n",
        "\n",
        "    # (선택) 모델 학습이 완료된 후 wandb를 종료합니다.\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DMS60wL-Dhv"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main(loaded_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFtWqowCGzEc"
      },
      "source": [
        "## 4. 모델 추론하기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhA8MXuAhH1W"
      },
      "outputs": [],
      "source": [
        "# 이곳에 내가 사용할 wandb config 설정\n",
        "loaded_config['inference']['ckt_path'] = \"추론에 사용할 ckt 경로 설정\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFGul3-rSscf"
      },
      "source": [
        "- test data를 사용하여 모델의 성능을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lV1Do7nlTylG"
      },
      "outputs": [],
      "source": [
        "# tokenization 과정까지 진행된 최종적으로 모델에 입력될 데이터를 출력합니다.\n",
        "def prepare_test_dataset(config,preprocessor, tokenizer):\n",
        "\n",
        "    test_file_path = os.path.join(config['general']['data_path'],'test.csv')\n",
        "\n",
        "    test_data = preprocessor.make_set_as_df(test_file_path,is_train=False)\n",
        "    test_id = test_data['fname']\n",
        "\n",
        "    print('-'*150)\n",
        "    print(f'test_data:\\n{test_data[\"dialogue\"][0]}')\n",
        "    print('-'*150)\n",
        "\n",
        "    encoder_input_test , decoder_input_test = preprocessor.make_input(test_data,is_test=True)\n",
        "    print('-'*10, 'Load data complete', '-'*10,)\n",
        "\n",
        "    test_tokenized_encoder_inputs = tokenizer(encoder_input_test, return_tensors=\"pt\", padding=True,\n",
        "                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False,)\n",
        "    test_tokenized_decoder_inputs = tokenizer(decoder_input_test, return_tensors=\"pt\", padding=True,\n",
        "                    add_special_tokens=True, truncation=True, max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False,)\n",
        "\n",
        "    test_encoder_inputs_dataset = DatasetForInference(test_tokenized_encoder_inputs, test_id, len(encoder_input_test))\n",
        "    print('-'*10, 'Make dataset complete', '-'*10,)\n",
        "\n",
        "    return test_data, test_encoder_inputs_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb49bLULT3aS"
      },
      "outputs": [],
      "source": [
        "# 추론을 위한 tokenizer와 학습시킨 모델을 불러옵니다.\n",
        "def load_tokenizer_and_model_for_test(config,device):\n",
        "    print('-'*10, 'Load tokenizer & model', '-'*10,)\n",
        "\n",
        "    model_name = config['general']['model_name']\n",
        "    ckt_path = config['inference']['ckt_path']\n",
        "    print('-'*10, f'Model Name : {model_name}', '-'*10,)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n",
        "    tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "    generate_model = BartForConditionalGeneration.from_pretrained(ckt_path)\n",
        "    generate_model.resize_token_embeddings(len(tokenizer))\n",
        "    generate_model.to(device)\n",
        "    print('-'*10, 'Load tokenizer & model complete', '-'*10,)\n",
        "\n",
        "    return generate_model , tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Axzu9rsoGLgJ"
      },
      "outputs": [],
      "source": [
        "# 학습된 모델이 생성한 요약문의 출력 결과를 보여줍니다.\n",
        "def inference(config):\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available()  else 'cpu')\n",
        "    print('-'*10, f'device : {device}', '-'*10,)\n",
        "    print(torch.__version__)\n",
        "\n",
        "    generate_model , tokenizer = load_tokenizer_and_model_for_test(config,device)\n",
        "\n",
        "    data_path = config['general']['data_path']\n",
        "    preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])\n",
        "\n",
        "    test_data, test_encoder_inputs_dataset = prepare_test_dataset(config,preprocessor, tokenizer)\n",
        "    dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=config['inference']['batch_size'])\n",
        "\n",
        "    summary = []\n",
        "    text_ids = []\n",
        "    with torch.no_grad():\n",
        "        for item in tqdm(dataloader):\n",
        "            text_ids.extend(item['ID'])\n",
        "            generated_ids = generate_model.generate(input_ids=item['input_ids'].to('cuda:0'),\n",
        "                            no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],\n",
        "                            early_stopping=config['inference']['early_stopping'],\n",
        "                            max_length=config['inference']['generate_max_length'],\n",
        "                            num_beams=config['inference']['num_beams'],\n",
        "                        )\n",
        "            for ids in generated_ids:\n",
        "                result = tokenizer.decode(ids)\n",
        "                summary.append(result)\n",
        "\n",
        "    # 정확한 평가를 위하여 노이즈에 해당되는 스페셜 토큰을 제거합니다.\n",
        "    remove_tokens = config['inference']['remove_tokens']\n",
        "    preprocessed_summary = summary.copy()\n",
        "    for token in remove_tokens:\n",
        "        preprocessed_summary = [sentence.replace(token,\" \") for sentence in preprocessed_summary]\n",
        "\n",
        "    output = pd.DataFrame(\n",
        "        {\n",
        "            \"fname\": test_data['fname'],\n",
        "            \"summary\" : preprocessed_summary,\n",
        "        }\n",
        "    )\n",
        "    result_path = config['inference']['result_path']\n",
        "    if not os.path.exists(result_path):\n",
        "        os.makedirs(result_path)\n",
        "    output.to_csv(os.path.join(result_path, \"output.csv\"), index=False)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pJ1ZXf-5V50"
      },
      "outputs": [],
      "source": [
        "# 학습된 모델의 test를 진행합니다.\n",
        "if __name__ == \"__main__\":\n",
        "    output = inference(loaded_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsPmLfhbzZqS"
      },
      "outputs": [],
      "source": [
        "output  # 각 대화문에 대한 요약문이 출력됨을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFLi_YL2hH12"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}